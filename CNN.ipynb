{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchmetrics import Accuracy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подгружаем данные и модель Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44898, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text  target  \n",
       "0  Donald Trump just couldn t wish all Americans ...       0  \n",
       "1  House Intelligence Committee Chairman Devin Nu...       0  \n",
       "2  On Friday, it was revealed that former Milwauk...       0  \n",
       "3  On Christmas day, Donald Trump announced that ...       0  \n",
       "4  Pope Francis used his annual Christmas Day mes...       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fake = pd.read_csv('data/Fake.csv')\n",
    "df_true = pd.read_csv('data/True.csv')\n",
    "df_fake['target'] = 0\n",
    "df_true['target'] = 1\n",
    "df = pd.concat([df_fake, df_true], ignore_index=True)\n",
    "df = df.drop(['subject', 'date'], axis=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32257"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Загрузим модель W2V\n",
    "model_w2v = Word2Vec.load('output/models/model_w2v.model')\n",
    "len(model_w2v.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создадим датасеты для train и valid, а также сделаем CustomDataloader для нейронки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31428, 3), (13470, 3))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_valid = train_test_split(df, test_size=0.3, train_size=0.7, random_state=42, shuffle=True)\n",
    "df_train.shape, df_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test tokenize raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text_: str)->list:\n",
    "        text_ = text_.replace(\"\\n\", \" \") \n",
    "        tokenized_text = []\n",
    "        # Итерируемся по каждому предложению в фалйе\n",
    "        for i in sent_tokenize(text_):\n",
    "            temp = []\n",
    "            # Токенизируем предложения в слова\n",
    "            sentence = i.lower()\n",
    "            sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "            sentence=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",sentence)\n",
    "            sentence=re.sub(\"(\\\\d|\\\\W)+\",\" \",sentence)\n",
    "            \n",
    "            for j in word_tokenize(sentence):\n",
    "                temp.append(j.lower())\n",
    "        \n",
    "            tokenized_text.append(temp)\n",
    "        return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(row: pd.Series):\n",
    "    series_list = []\n",
    "    for idx in row.index:\n",
    "        tokens = tokenize(row[idx])\n",
    "        vector = np.zeros(model_w2v.vector_size)\n",
    "        for sentence in tokens:\n",
    "            for word in sentence:\n",
    "                try:\n",
    "                    vector += model_w2v.wv.get_vector(word)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        features = pd.Series(data=vector, index=[idx+f'_{i}' for i in range(len(vector))])\n",
    "        series_list.append(features)\n",
    "        \n",
    "    return pd.concat(series_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_0</th>\n",
       "      <th>title_1</th>\n",
       "      <th>title_2</th>\n",
       "      <th>title_3</th>\n",
       "      <th>title_4</th>\n",
       "      <th>title_5</th>\n",
       "      <th>title_6</th>\n",
       "      <th>title_7</th>\n",
       "      <th>title_8</th>\n",
       "      <th>title_9</th>\n",
       "      <th>...</th>\n",
       "      <th>text_10</th>\n",
       "      <th>text_11</th>\n",
       "      <th>text_12</th>\n",
       "      <th>text_13</th>\n",
       "      <th>text_14</th>\n",
       "      <th>text_15</th>\n",
       "      <th>text_16</th>\n",
       "      <th>text_17</th>\n",
       "      <th>text_18</th>\n",
       "      <th>text_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13970</th>\n",
       "      <td>-20.561163</td>\n",
       "      <td>1.705486</td>\n",
       "      <td>-2.813859</td>\n",
       "      <td>8.285939</td>\n",
       "      <td>-9.937233</td>\n",
       "      <td>36.796398</td>\n",
       "      <td>-0.300022</td>\n",
       "      <td>5.273900</td>\n",
       "      <td>5.686247</td>\n",
       "      <td>1.478673</td>\n",
       "      <td>...</td>\n",
       "      <td>-207.849375</td>\n",
       "      <td>-399.984856</td>\n",
       "      <td>128.510403</td>\n",
       "      <td>-106.046970</td>\n",
       "      <td>-1340.851631</td>\n",
       "      <td>332.768740</td>\n",
       "      <td>595.763102</td>\n",
       "      <td>-295.093025</td>\n",
       "      <td>-601.274971</td>\n",
       "      <td>-337.852130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41668</th>\n",
       "      <td>-39.902468</td>\n",
       "      <td>12.084899</td>\n",
       "      <td>-1.709602</td>\n",
       "      <td>-13.218677</td>\n",
       "      <td>-21.592071</td>\n",
       "      <td>-4.526084</td>\n",
       "      <td>-21.932969</td>\n",
       "      <td>1.423887</td>\n",
       "      <td>-1.258491</td>\n",
       "      <td>4.497511</td>\n",
       "      <td>...</td>\n",
       "      <td>79.655821</td>\n",
       "      <td>-28.165098</td>\n",
       "      <td>-99.042810</td>\n",
       "      <td>25.112562</td>\n",
       "      <td>-217.632894</td>\n",
       "      <td>-113.776709</td>\n",
       "      <td>-8.500680</td>\n",
       "      <td>-127.412714</td>\n",
       "      <td>-324.369708</td>\n",
       "      <td>-84.265618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26810</th>\n",
       "      <td>-48.529369</td>\n",
       "      <td>16.988462</td>\n",
       "      <td>0.704686</td>\n",
       "      <td>-2.346718</td>\n",
       "      <td>-13.877482</td>\n",
       "      <td>-7.049728</td>\n",
       "      <td>-13.525976</td>\n",
       "      <td>10.662228</td>\n",
       "      <td>13.997027</td>\n",
       "      <td>-3.210051</td>\n",
       "      <td>...</td>\n",
       "      <td>511.014646</td>\n",
       "      <td>298.127093</td>\n",
       "      <td>414.678216</td>\n",
       "      <td>103.258333</td>\n",
       "      <td>-384.669279</td>\n",
       "      <td>-409.957141</td>\n",
       "      <td>268.284043</td>\n",
       "      <td>-191.122515</td>\n",
       "      <td>-542.920393</td>\n",
       "      <td>70.823093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30967</th>\n",
       "      <td>-1.258231</td>\n",
       "      <td>14.229122</td>\n",
       "      <td>16.629794</td>\n",
       "      <td>5.558938</td>\n",
       "      <td>6.001331</td>\n",
       "      <td>38.216741</td>\n",
       "      <td>27.653911</td>\n",
       "      <td>6.158491</td>\n",
       "      <td>7.341925</td>\n",
       "      <td>2.906823</td>\n",
       "      <td>...</td>\n",
       "      <td>491.475686</td>\n",
       "      <td>-181.986034</td>\n",
       "      <td>67.300425</td>\n",
       "      <td>-266.289010</td>\n",
       "      <td>-209.300461</td>\n",
       "      <td>-355.221548</td>\n",
       "      <td>287.658472</td>\n",
       "      <td>-63.994527</td>\n",
       "      <td>144.846943</td>\n",
       "      <td>316.676905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26072</th>\n",
       "      <td>-42.545065</td>\n",
       "      <td>-6.151385</td>\n",
       "      <td>-44.947698</td>\n",
       "      <td>9.563522</td>\n",
       "      <td>-18.862579</td>\n",
       "      <td>-32.311658</td>\n",
       "      <td>4.719734</td>\n",
       "      <td>3.072655</td>\n",
       "      <td>15.712891</td>\n",
       "      <td>19.545215</td>\n",
       "      <td>...</td>\n",
       "      <td>-105.818652</td>\n",
       "      <td>76.588979</td>\n",
       "      <td>608.738456</td>\n",
       "      <td>488.648363</td>\n",
       "      <td>-525.601861</td>\n",
       "      <td>-112.235006</td>\n",
       "      <td>-9.093687</td>\n",
       "      <td>-1085.552581</td>\n",
       "      <td>-109.915153</td>\n",
       "      <td>-267.367391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7209</th>\n",
       "      <td>-26.003996</td>\n",
       "      <td>3.622691</td>\n",
       "      <td>19.844227</td>\n",
       "      <td>32.363963</td>\n",
       "      <td>7.756209</td>\n",
       "      <td>-20.215562</td>\n",
       "      <td>8.282713</td>\n",
       "      <td>12.835868</td>\n",
       "      <td>19.285243</td>\n",
       "      <td>-2.806621</td>\n",
       "      <td>...</td>\n",
       "      <td>230.882876</td>\n",
       "      <td>-31.972784</td>\n",
       "      <td>604.556259</td>\n",
       "      <td>264.453843</td>\n",
       "      <td>-518.395541</td>\n",
       "      <td>66.800142</td>\n",
       "      <td>264.832368</td>\n",
       "      <td>-433.785902</td>\n",
       "      <td>302.314514</td>\n",
       "      <td>265.590722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8575</th>\n",
       "      <td>-5.880494</td>\n",
       "      <td>21.249021</td>\n",
       "      <td>-1.470611</td>\n",
       "      <td>25.700341</td>\n",
       "      <td>-25.608693</td>\n",
       "      <td>19.329410</td>\n",
       "      <td>-8.853274</td>\n",
       "      <td>-21.062501</td>\n",
       "      <td>-10.993270</td>\n",
       "      <td>58.789575</td>\n",
       "      <td>...</td>\n",
       "      <td>191.248024</td>\n",
       "      <td>-392.947424</td>\n",
       "      <td>-14.272134</td>\n",
       "      <td>-65.698409</td>\n",
       "      <td>-516.617200</td>\n",
       "      <td>339.016040</td>\n",
       "      <td>450.008702</td>\n",
       "      <td>-9.012203</td>\n",
       "      <td>560.351363</td>\n",
       "      <td>112.212919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41384</th>\n",
       "      <td>-20.171666</td>\n",
       "      <td>-18.079981</td>\n",
       "      <td>6.122062</td>\n",
       "      <td>-10.063215</td>\n",
       "      <td>-2.608655</td>\n",
       "      <td>-0.822229</td>\n",
       "      <td>-16.906326</td>\n",
       "      <td>10.794742</td>\n",
       "      <td>10.210679</td>\n",
       "      <td>26.092180</td>\n",
       "      <td>...</td>\n",
       "      <td>161.258695</td>\n",
       "      <td>-236.590131</td>\n",
       "      <td>134.691902</td>\n",
       "      <td>-9.783641</td>\n",
       "      <td>38.266353</td>\n",
       "      <td>-140.475202</td>\n",
       "      <td>308.873206</td>\n",
       "      <td>-456.213430</td>\n",
       "      <td>-660.571145</td>\n",
       "      <td>92.515608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34361</th>\n",
       "      <td>-22.001598</td>\n",
       "      <td>-20.279675</td>\n",
       "      <td>-16.319980</td>\n",
       "      <td>18.673759</td>\n",
       "      <td>14.521915</td>\n",
       "      <td>-16.555194</td>\n",
       "      <td>15.947908</td>\n",
       "      <td>-13.456891</td>\n",
       "      <td>30.764410</td>\n",
       "      <td>19.614207</td>\n",
       "      <td>...</td>\n",
       "      <td>116.012522</td>\n",
       "      <td>452.386276</td>\n",
       "      <td>577.084155</td>\n",
       "      <td>218.051598</td>\n",
       "      <td>-421.430609</td>\n",
       "      <td>-458.197249</td>\n",
       "      <td>264.858352</td>\n",
       "      <td>-712.613166</td>\n",
       "      <td>383.070314</td>\n",
       "      <td>631.153352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29971</th>\n",
       "      <td>6.600970</td>\n",
       "      <td>1.272936</td>\n",
       "      <td>-11.103766</td>\n",
       "      <td>-26.819343</td>\n",
       "      <td>-4.800324</td>\n",
       "      <td>0.352383</td>\n",
       "      <td>28.931379</td>\n",
       "      <td>-26.712205</td>\n",
       "      <td>-10.233462</td>\n",
       "      <td>11.537558</td>\n",
       "      <td>...</td>\n",
       "      <td>1153.336848</td>\n",
       "      <td>821.638676</td>\n",
       "      <td>334.598121</td>\n",
       "      <td>-711.812696</td>\n",
       "      <td>-109.027652</td>\n",
       "      <td>-847.780918</td>\n",
       "      <td>627.253900</td>\n",
       "      <td>-638.205925</td>\n",
       "      <td>-886.325826</td>\n",
       "      <td>228.381205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         title_0    title_1    title_2    title_3    title_4    title_5  \\\n",
       "13970 -20.561163   1.705486  -2.813859   8.285939  -9.937233  36.796398   \n",
       "41668 -39.902468  12.084899  -1.709602 -13.218677 -21.592071  -4.526084   \n",
       "26810 -48.529369  16.988462   0.704686  -2.346718 -13.877482  -7.049728   \n",
       "30967  -1.258231  14.229122  16.629794   5.558938   6.001331  38.216741   \n",
       "26072 -42.545065  -6.151385 -44.947698   9.563522 -18.862579 -32.311658   \n",
       "7209  -26.003996   3.622691  19.844227  32.363963   7.756209 -20.215562   \n",
       "8575   -5.880494  21.249021  -1.470611  25.700341 -25.608693  19.329410   \n",
       "41384 -20.171666 -18.079981   6.122062 -10.063215  -2.608655  -0.822229   \n",
       "34361 -22.001598 -20.279675 -16.319980  18.673759  14.521915 -16.555194   \n",
       "29971   6.600970   1.272936 -11.103766 -26.819343  -4.800324   0.352383   \n",
       "\n",
       "         title_6    title_7    title_8    title_9  ...      text_10  \\\n",
       "13970  -0.300022   5.273900   5.686247   1.478673  ...  -207.849375   \n",
       "41668 -21.932969   1.423887  -1.258491   4.497511  ...    79.655821   \n",
       "26810 -13.525976  10.662228  13.997027  -3.210051  ...   511.014646   \n",
       "30967  27.653911   6.158491   7.341925   2.906823  ...   491.475686   \n",
       "26072   4.719734   3.072655  15.712891  19.545215  ...  -105.818652   \n",
       "7209    8.282713  12.835868  19.285243  -2.806621  ...   230.882876   \n",
       "8575   -8.853274 -21.062501 -10.993270  58.789575  ...   191.248024   \n",
       "41384 -16.906326  10.794742  10.210679  26.092180  ...   161.258695   \n",
       "34361  15.947908 -13.456891  30.764410  19.614207  ...   116.012522   \n",
       "29971  28.931379 -26.712205 -10.233462  11.537558  ...  1153.336848   \n",
       "\n",
       "          text_11     text_12     text_13      text_14     text_15  \\\n",
       "13970 -399.984856  128.510403 -106.046970 -1340.851631  332.768740   \n",
       "41668  -28.165098  -99.042810   25.112562  -217.632894 -113.776709   \n",
       "26810  298.127093  414.678216  103.258333  -384.669279 -409.957141   \n",
       "30967 -181.986034   67.300425 -266.289010  -209.300461 -355.221548   \n",
       "26072   76.588979  608.738456  488.648363  -525.601861 -112.235006   \n",
       "7209   -31.972784  604.556259  264.453843  -518.395541   66.800142   \n",
       "8575  -392.947424  -14.272134  -65.698409  -516.617200  339.016040   \n",
       "41384 -236.590131  134.691902   -9.783641    38.266353 -140.475202   \n",
       "34361  452.386276  577.084155  218.051598  -421.430609 -458.197249   \n",
       "29971  821.638676  334.598121 -711.812696  -109.027652 -847.780918   \n",
       "\n",
       "          text_16      text_17     text_18     text_19  \n",
       "13970  595.763102  -295.093025 -601.274971 -337.852130  \n",
       "41668   -8.500680  -127.412714 -324.369708  -84.265618  \n",
       "26810  268.284043  -191.122515 -542.920393   70.823093  \n",
       "30967  287.658472   -63.994527  144.846943  316.676905  \n",
       "26072   -9.093687 -1085.552581 -109.915153 -267.367391  \n",
       "7209   264.832368  -433.785902  302.314514  265.590722  \n",
       "8575   450.008702    -9.012203  560.351363  112.212919  \n",
       "41384  308.873206  -456.213430 -660.571145   92.515608  \n",
       "34361  264.858352  -712.613166  383.070314  631.153352  \n",
       "29971  627.253900  -638.205925 -886.325826  228.381205  \n",
       "\n",
       "[10 rows x 40 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[['title', 'text']].head(10).apply(vectorize_text, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_: pd.DataFrame, model_w2v_: Word2Vec):\n",
    "        \n",
    "        self.model = model_w2v_\n",
    "        self.data = data_.reset_index(drop=True)\n",
    "        \n",
    "        self.vecorized_data = self.data[['title', 'text']].apply(self.vectorize_text, axis=1)\n",
    "        self.target = self.data['target']\n",
    "        self.columns = self.vecorized_data.columns.values\n",
    "        return\n",
    "    \n",
    "    def tokenize(self, text_: str)->list:\n",
    "        text_ = text_.replace(\"\\n\", \" \") \n",
    "        tokenized_text = []\n",
    "        # Итерируемся по каждому предложению в фалйе\n",
    "        for i in sent_tokenize(text_):\n",
    "            temp = []\n",
    "            # Токенизируем предложения в слова\n",
    "            sentence = i.lower()\n",
    "            sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "            sentence=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",sentence)\n",
    "            sentence=re.sub(\"(\\\\d|\\\\W)+\",\" \",sentence)\n",
    "            \n",
    "            for j in word_tokenize(sentence):\n",
    "                temp.append(j.lower())\n",
    "        \n",
    "            tokenized_text.append(temp)\n",
    "        return tokenized_text\n",
    "    \n",
    "    def vectorize_text(self, row: pd.Series):\n",
    "        series_list = []\n",
    "        for idx in row.index:\n",
    "            tokens = self.tokenize(row[idx])\n",
    "            vector = np.zeros(self.model.vector_size)\n",
    "            for sentence in tokens:\n",
    "                for word in sentence:\n",
    "                    try:\n",
    "                        vector += self.model.wv.get_vector(word)\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            features = pd.Series(data=vector, index=[idx+f'_{i}' for i in range(len(vector))])\n",
    "            series_list.append(features)\n",
    "            \n",
    "        return pd.concat(series_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vecorized_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #row = self.vecorized_data.take([idx], axis=0)\n",
    "        #row = {col: torch.tensor(row[col].values, dtype=torch.float32) for i, col in enumerate(self.columns)}\n",
    "        row = self.vecorized_data.iloc[idx].values\n",
    "        return row, np.float32(self.target[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create train dataset\n"
     ]
    }
   ],
   "source": [
    "#TRAIN\n",
    "train_dataset = CustomDataset(df_train, model_w2v)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "print('Create train dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create test dataset\n"
     ]
    }
   ],
   "source": [
    "#VALID\n",
    "valid_dataset = CustomDataset(df_valid, model_w2v)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "print('Create test dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save tokenized texts\n",
    "with open('output/train_loader.pkl', 'wb') as f:\n",
    "    pickle.dump(train_loader, f)\n",
    "\n",
    "#Save tokenized texts\n",
    "with open('output/valid_loader.pkl', 'wb') as f:\n",
    "    pickle.dump(valid_loader, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load tokenized texts\n",
    "with open('output/train_loader.pkl', 'rb') as f:\n",
    "    loaded_train_loader = pickle.load(f)\n",
    "\n",
    "#Load tokenized texts\n",
    "with open('output/valid_loader.pkl', 'rb') as f:\n",
    "    loaded_valid_loader = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, kernel=3, len_features=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(BATCH_SIZE, BATCH_SIZE, kernel)\n",
    "        self.bn1 = nn.BatchNorm1d(len_features-kernel+1)\n",
    "        self.fc1 = nn.Linear(len_features-kernel+1, int(len_features/2))\n",
    "        self.fc2 = nn.Linear(int(len_features/2), int(len_features/4))\n",
    "        self.fc3 = nn.Linear(int(len_features/4), 2)\n",
    "        self.soft_max = nn.Softmax(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.soft_max(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv1d(64, 64, kernel_size=(10,), stride=(1,))\n",
      "  (bn1): BatchNorm1d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=31, out_features=20, bias=True)\n",
      "  (fc2): Linear(in_features=20, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (soft_max): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haze1\\AppData\\Local\\Temp\\ipykernel_16252\\2825255124.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  acc = accuracy(torch.argmin(output, dim=1).cpu(), torch.tensor(label, dtype=torch.int64)).item()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 0 STEP 50 : train_loss: 0.681552 train_acc: 0.593750\n",
      "\n",
      "EPOCH 0 STEP 100 : train_loss: 0.693022 train_acc: 0.500000\n",
      "\n",
      "EPOCH 0 STEP 150 : train_loss: 0.672920 train_acc: 0.593750\n",
      "\n",
      "EPOCH 0 STEP 200 : train_loss: 0.704829 train_acc: 0.406250\n",
      "\n",
      "EPOCH 0 STEP 250 : train_loss: 0.681679 train_acc: 0.562500\n",
      "\n",
      "EPOCH 0 STEP 300 : train_loss: 0.652214 train_acc: 0.656250\n",
      "\n",
      "EPOCH 0 STEP 350 : train_loss: 0.645225 train_acc: 0.609375\n",
      "\n",
      "EPOCH 0 STEP 400 : train_loss: 0.635160 train_acc: 0.593750\n",
      "\n",
      "EPOCH 0 STEP 450 : train_loss: 0.678392 train_acc: 0.562500\n",
      "valid_loss: 0.577851 valid_acc: 0.697321\n",
      "\n",
      "EPOCH 1 STEP 500 : train_loss: 0.517160 train_acc: 0.765625\n",
      "\n",
      "EPOCH 1 STEP 550 : train_loss: 0.509399 train_acc: 0.718750\n",
      "\n",
      "EPOCH 1 STEP 600 : train_loss: 0.551452 train_acc: 0.718750\n",
      "\n",
      "EPOCH 1 STEP 650 : train_loss: 0.498925 train_acc: 0.734375\n",
      "\n",
      "EPOCH 1 STEP 700 : train_loss: 0.523480 train_acc: 0.750000\n",
      "\n",
      "EPOCH 1 STEP 750 : train_loss: 0.556316 train_acc: 0.718750\n",
      "\n",
      "EPOCH 1 STEP 800 : train_loss: 0.520104 train_acc: 0.718750\n",
      "\n",
      "EPOCH 1 STEP 850 : train_loss: 0.430584 train_acc: 0.781250\n",
      "\n",
      "EPOCH 1 STEP 900 : train_loss: 0.540189 train_acc: 0.703125\n",
      "\n",
      "EPOCH 1 STEP 950 : train_loss: 0.424405 train_acc: 0.796875\n",
      "valid_loss: 0.441144 valid_acc: 0.796131\n",
      "\n",
      "EPOCH 2 STEP 1000 : train_loss: 0.442186 train_acc: 0.843750\n",
      "\n",
      "EPOCH 2 STEP 1050 : train_loss: 0.432167 train_acc: 0.750000\n",
      "\n",
      "EPOCH 2 STEP 1100 : train_loss: 0.567876 train_acc: 0.734375\n",
      "\n",
      "EPOCH 2 STEP 1150 : train_loss: 0.496478 train_acc: 0.703125\n",
      "\n",
      "EPOCH 2 STEP 1200 : train_loss: 0.352829 train_acc: 0.796875\n",
      "\n",
      "EPOCH 2 STEP 1250 : train_loss: 0.299253 train_acc: 0.906250\n",
      "\n",
      "EPOCH 2 STEP 1300 : train_loss: 0.546032 train_acc: 0.812500\n",
      "\n",
      "EPOCH 2 STEP 1350 : train_loss: 0.325505 train_acc: 0.859375\n",
      "\n",
      "EPOCH 2 STEP 1400 : train_loss: 0.458536 train_acc: 0.828125\n",
      "\n",
      "EPOCH 2 STEP 1450 : train_loss: 0.429292 train_acc: 0.765625\n",
      "valid_loss: 0.378775 valid_acc: 0.833705\n",
      "\n",
      "EPOCH 3 STEP 1500 : train_loss: 0.259436 train_acc: 0.875000\n",
      "\n",
      "EPOCH 3 STEP 1550 : train_loss: 0.281854 train_acc: 0.937500\n",
      "\n",
      "EPOCH 3 STEP 1600 : train_loss: 0.307707 train_acc: 0.890625\n",
      "\n",
      "EPOCH 3 STEP 1650 : train_loss: 0.385707 train_acc: 0.843750\n",
      "\n",
      "EPOCH 3 STEP 1700 : train_loss: 0.389153 train_acc: 0.812500\n",
      "\n",
      "EPOCH 3 STEP 1750 : train_loss: 0.272585 train_acc: 0.890625\n",
      "\n",
      "EPOCH 3 STEP 1800 : train_loss: 0.379766 train_acc: 0.812500\n",
      "\n",
      "EPOCH 3 STEP 1850 : train_loss: 0.564822 train_acc: 0.781250\n",
      "\n",
      "EPOCH 3 STEP 1900 : train_loss: 0.288513 train_acc: 0.890625\n",
      "\n",
      "EPOCH 3 STEP 1950 : train_loss: 0.305953 train_acc: 0.859375\n",
      "valid_loss: 0.353487 valid_acc: 0.844196\n",
      "\n",
      "EPOCH 4 STEP 2000 : train_loss: 0.252271 train_acc: 0.921875\n",
      "\n",
      "EPOCH 4 STEP 2050 : train_loss: 0.418468 train_acc: 0.765625\n",
      "\n",
      "EPOCH 4 STEP 2100 : train_loss: 0.254738 train_acc: 0.890625\n",
      "\n",
      "EPOCH 4 STEP 2150 : train_loss: 0.408714 train_acc: 0.843750\n",
      "\n",
      "EPOCH 4 STEP 2200 : train_loss: 0.328281 train_acc: 0.875000\n",
      "\n",
      "EPOCH 4 STEP 2250 : train_loss: 0.353825 train_acc: 0.812500\n",
      "\n",
      "EPOCH 4 STEP 2300 : train_loss: 0.321213 train_acc: 0.828125\n",
      "\n",
      "EPOCH 4 STEP 2350 : train_loss: 0.441437 train_acc: 0.796875\n",
      "\n",
      "EPOCH 4 STEP 2400 : train_loss: 0.351688 train_acc: 0.812500\n",
      "\n",
      "EPOCH 4 STEP 2450 : train_loss: 0.287296 train_acc: 0.921875\n",
      "valid_loss: 0.328046 valid_acc: 0.857738\n",
      "\n",
      "EPOCH 5 STEP 2500 : train_loss: 0.322209 train_acc: 0.843750\n",
      "\n",
      "EPOCH 5 STEP 2550 : train_loss: 0.355493 train_acc: 0.875000\n",
      "\n",
      "EPOCH 5 STEP 2600 : train_loss: 0.342852 train_acc: 0.843750\n",
      "\n",
      "EPOCH 5 STEP 2650 : train_loss: 0.318733 train_acc: 0.843750\n",
      "\n",
      "EPOCH 5 STEP 2700 : train_loss: 0.332228 train_acc: 0.859375\n",
      "\n",
      "EPOCH 5 STEP 2750 : train_loss: 0.439814 train_acc: 0.843750\n",
      "\n",
      "EPOCH 5 STEP 2800 : train_loss: 0.275536 train_acc: 0.921875\n",
      "\n",
      "EPOCH 5 STEP 2850 : train_loss: 0.270897 train_acc: 0.906250\n",
      "\n",
      "EPOCH 5 STEP 2900 : train_loss: 0.272530 train_acc: 0.921875\n",
      "valid_loss: 0.308599 valid_acc: 0.866667\n",
      "\n",
      "EPOCH 6 STEP 2950 : train_loss: 0.254392 train_acc: 0.890625\n",
      "\n",
      "EPOCH 6 STEP 3000 : train_loss: 0.231887 train_acc: 0.890625\n",
      "\n",
      "EPOCH 6 STEP 3050 : train_loss: 0.445272 train_acc: 0.796875\n",
      "\n",
      "EPOCH 6 STEP 3100 : train_loss: 0.315225 train_acc: 0.875000\n",
      "\n",
      "EPOCH 6 STEP 3150 : train_loss: 0.187675 train_acc: 0.953125\n",
      "\n",
      "EPOCH 6 STEP 3200 : train_loss: 0.243841 train_acc: 0.921875\n",
      "\n",
      "EPOCH 6 STEP 3250 : train_loss: 0.425760 train_acc: 0.812500\n",
      "\n",
      "EPOCH 6 STEP 3300 : train_loss: 0.444374 train_acc: 0.859375\n",
      "\n",
      "EPOCH 6 STEP 3350 : train_loss: 0.237858 train_acc: 0.875000\n",
      "\n",
      "EPOCH 6 STEP 3400 : train_loss: 0.361931 train_acc: 0.812500\n",
      "valid_loss: 0.298417 valid_acc: 0.874107\n",
      "\n",
      "EPOCH 7 STEP 3450 : train_loss: 0.316434 train_acc: 0.921875\n",
      "\n",
      "EPOCH 7 STEP 3500 : train_loss: 0.495453 train_acc: 0.765625\n",
      "\n",
      "EPOCH 7 STEP 3550 : train_loss: 0.352395 train_acc: 0.859375\n",
      "\n",
      "EPOCH 7 STEP 3600 : train_loss: 0.383191 train_acc: 0.875000\n",
      "\n",
      "EPOCH 7 STEP 3650 : train_loss: 0.352004 train_acc: 0.828125\n",
      "\n",
      "EPOCH 7 STEP 3700 : train_loss: 0.416516 train_acc: 0.812500\n",
      "\n",
      "EPOCH 7 STEP 3750 : train_loss: 0.298682 train_acc: 0.843750\n",
      "\n",
      "EPOCH 7 STEP 3800 : train_loss: 0.278391 train_acc: 0.859375\n",
      "\n",
      "EPOCH 7 STEP 3850 : train_loss: 0.349044 train_acc: 0.859375\n",
      "\n",
      "EPOCH 7 STEP 3900 : train_loss: 0.353070 train_acc: 0.890625\n",
      "valid_loss: 0.298184 valid_acc: 0.875000\n",
      "\n",
      "EPOCH 8 STEP 3950 : train_loss: 0.335199 train_acc: 0.828125\n",
      "\n",
      "EPOCH 8 STEP 4000 : train_loss: 0.393708 train_acc: 0.859375\n",
      "\n",
      "EPOCH 8 STEP 4050 : train_loss: 0.341033 train_acc: 0.843750\n",
      "\n",
      "EPOCH 8 STEP 4100 : train_loss: 0.242584 train_acc: 0.921875\n",
      "\n",
      "EPOCH 8 STEP 4150 : train_loss: 0.260753 train_acc: 0.906250\n",
      "\n",
      "EPOCH 8 STEP 4200 : train_loss: 0.335081 train_acc: 0.875000\n",
      "\n",
      "EPOCH 8 STEP 4250 : train_loss: 0.241546 train_acc: 0.875000\n",
      "\n",
      "EPOCH 8 STEP 4300 : train_loss: 0.218160 train_acc: 0.890625\n",
      "\n",
      "EPOCH 8 STEP 4350 : train_loss: 0.470482 train_acc: 0.828125\n",
      "\n",
      "EPOCH 8 STEP 4400 : train_loss: 0.389737 train_acc: 0.859375\n",
      "valid_loss: 0.286314 valid_acc: 0.878571\n",
      "\n",
      "EPOCH 9 STEP 4450 : train_loss: 0.375051 train_acc: 0.828125\n",
      "\n",
      "EPOCH 9 STEP 4500 : train_loss: 0.211432 train_acc: 0.953125\n",
      "\n",
      "EPOCH 9 STEP 4550 : train_loss: 0.292793 train_acc: 0.906250\n",
      "\n",
      "EPOCH 9 STEP 4600 : train_loss: 0.347708 train_acc: 0.796875\n",
      "\n",
      "EPOCH 9 STEP 4650 : train_loss: 0.326221 train_acc: 0.906250\n",
      "\n",
      "EPOCH 9 STEP 4700 : train_loss: 0.268178 train_acc: 0.890625\n",
      "\n",
      "EPOCH 9 STEP 4750 : train_loss: 0.150838 train_acc: 0.968750\n",
      "\n",
      "EPOCH 9 STEP 4800 : train_loss: 0.151261 train_acc: 0.953125\n",
      "\n",
      "EPOCH 9 STEP 4850 : train_loss: 0.281883 train_acc: 0.859375\n",
      "\n",
      "EPOCH 9 STEP 4900 : train_loss: 0.367921 train_acc: 0.843750\n",
      "valid_loss: 0.282541 valid_acc: 0.880283\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "model_cnn = CNN(kernel=10, \n",
    "                len_features=len(loaded_train_loader.dataset.columns)).float().to(device=device)\n",
    "\n",
    "loss = torch.nn.BCELoss().to(device)\n",
    "accuracy = Accuracy(task='binary').to(device)\n",
    "optimizer = optim.Adam(model_cnn.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(model_cnn)\n",
    "\n",
    "step = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    model_cnn.train()\n",
    "    for features, label in loaded_train_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model_cnn(features.float().to(device))\n",
    "        \n",
    "        # Calculate error and backpropagate\n",
    "        loss_val = loss(output, torch.stack([label, ((label-1)*(-1)).int()]).T.to(device))\n",
    "        output = torch.sigmoid(output).to(device)\n",
    "        loss_val.backward()                \n",
    "        acc = accuracy(torch.argmin(output, dim=1).cpu(), torch.tensor(label, dtype=torch.int64)).item()\n",
    "        \n",
    "        # Update weights with gradients\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print('\\nEPOCH %d STEP %d : train_loss: %f train_acc: %f' %(epoch, step, loss_val.item(), acc))\n",
    "            #print(torch.argmin(output, dim=1).cpu(), torch.tensor(label, dtype=torch.int64), sep='\\n')\n",
    "            \n",
    "    # Run validation\n",
    "    running_loss = []\n",
    "    valid_scores = []\n",
    "    valid_labels = []\n",
    "    model_cnn.eval()\n",
    "    with torch.no_grad():\n",
    "        for features, label in valid_loader:\n",
    "            output = model_cnn(features.float().to(device))\n",
    "            # Calculate error and backpropagate\n",
    "            try:\n",
    "                loss_val = loss(output, torch.stack([label, ((label-1)*(-1)).int()]).T.to(device))\n",
    "            except ValueError:\n",
    "                print(output, label)\n",
    "            \n",
    "            running_loss.append(loss_val.item())\n",
    "            valid_scores.extend(torch.argmin(output, dim=1).cpu())\n",
    "            valid_labels.extend(label)\n",
    "\n",
    "    valid_accuracy = accuracy(torch.tensor(valid_scores), torch.tensor(valid_labels)).item()\n",
    "    print('valid_loss: %f valid_acc: %f' % (np.mean(running_loss), valid_accuracy))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text_: str)->list:\n",
    "        text_ = text_.replace(\"\\n\", \" \") \n",
    "        tokenized_text = []\n",
    "        # Итерируемся по каждому предложению в фалйе\n",
    "        for i in sent_tokenize(text_):\n",
    "            temp = []\n",
    "            # Токенизируем предложения в слова\n",
    "            sentence = i.lower()\n",
    "            sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "            sentence=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",sentence)\n",
    "            sentence=re.sub(\"(\\\\d|\\\\W)+\",\" \",sentence)\n",
    "            \n",
    "            for j in word_tokenize(sentence):\n",
    "                temp.append(j.lower())\n",
    "        \n",
    "            tokenized_text.append(temp)\n",
    "        return tokenized_text\n",
    "    \n",
    "def vectorize_text(row: pd.Series):\n",
    "    series_list = []\n",
    "    for idx in row.index:\n",
    "        tokens = tokenize(row[idx])\n",
    "        vector = np.zeros(model_w2v.vector_size)\n",
    "        for sentence in tokens:\n",
    "            for word in sentence:\n",
    "                try:\n",
    "                    vector += model_w2v.wv.get_vector(word)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        features = pd.Series(data=vector, index=[idx+f'_{i}' for i in range(len(vector))])\n",
    "        series_list.append(features)\n",
    "        \n",
    "    return pd.concat(series_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data: pd.DataFrame, model_cnn: nn.Module):\n",
    "    vecorized_data = data[['title', 'text']].apply(vectorize_text, axis=1)\n",
    "    input_vector = vecorized_data.values\n",
    "    pred = model_cnn.cpu().forward(torch.tensor(input_vector).float())\n",
    "    return torch.argmin(pred, dim=1).cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3873</th>\n",
       "      <td>Russia Brags About Helping Trump Win As Our E...</td>\n",
       "      <td>Russia and Vladimir Putin got what they wanted...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8555</th>\n",
       "      <td>SNL Parodies All-White Oscars In Brutal Smack...</td>\n",
       "      <td>Last night, Saturday Night Live delivered a br...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33716</th>\n",
       "      <td>Senate proposal on encryption gives judges bro...</td>\n",
       "      <td>WASHINGTON (Reuters) - A bipartisan group of U...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10692</th>\n",
       "      <td>DONALD TRUMP JR Slams Kathy Griffin for Playin...</td>\n",
       "      <td>Donald Trump Jr slammed Kathy Griffin for play...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12284</th>\n",
       "      <td>TRUMP ADVISOR: “If election results are overtu...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28271</th>\n",
       "      <td>Trump, Koch brothers at odds over 'Trumpcare' ...</td>\n",
       "      <td>WASHINGTON (Reuters) - Republicans considering...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30683</th>\n",
       "      <td>Merkel silent on fourth term despite glowing w...</td>\n",
       "      <td>BERLIN (Reuters) - U.S. President Barack Obama...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5475</th>\n",
       "      <td>A Whopping 0% Of Black Voters In Ohio And Pen...</td>\n",
       "      <td>That s right, according to the latest NBC News...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2226</th>\n",
       "      <td>Obama Just Gave The PERFECT Response To Donal...</td>\n",
       "      <td>When former President George W. Bush left offi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10564</th>\n",
       "      <td>SOCIAL MEDIA ERUPTS Over Awkward Photo Of Marc...</td>\n",
       "      <td>Sometimes all you can do is surrender to Twitt...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "3873    Russia Brags About Helping Trump Win As Our E...   \n",
       "8555    SNL Parodies All-White Oscars In Brutal Smack...   \n",
       "33716  Senate proposal on encryption gives judges bro...   \n",
       "10692  DONALD TRUMP JR Slams Kathy Griffin for Playin...   \n",
       "12284  TRUMP ADVISOR: “If election results are overtu...   \n",
       "...                                                  ...   \n",
       "28271  Trump, Koch brothers at odds over 'Trumpcare' ...   \n",
       "30683  Merkel silent on fourth term despite glowing w...   \n",
       "5475    A Whopping 0% Of Black Voters In Ohio And Pen...   \n",
       "2226    Obama Just Gave The PERFECT Response To Donal...   \n",
       "10564  SOCIAL MEDIA ERUPTS Over Awkward Photo Of Marc...   \n",
       "\n",
       "                                                    text  target  predict  \n",
       "3873   Russia and Vladimir Putin got what they wanted...       0        0  \n",
       "8555   Last night, Saturday Night Live delivered a br...       0        0  \n",
       "33716  WASHINGTON (Reuters) - A bipartisan group of U...       1        1  \n",
       "10692  Donald Trump Jr slammed Kathy Griffin for play...       0        0  \n",
       "12284                                                          0        0  \n",
       "...                                                  ...     ...      ...  \n",
       "28271  WASHINGTON (Reuters) - Republicans considering...       1        1  \n",
       "30683  BERLIN (Reuters) - U.S. President Barack Obama...       1        1  \n",
       "5475   That s right, according to the latest NBC News...       0        0  \n",
       "2226   When former President George W. Bush left offi...       0        0  \n",
       "10564  Sometimes all you can do is surrender to Twitt...       0        0  \n",
       "\n",
       "[64 rows x 4 columns]"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_test = df.sample(BATCH_SIZE)\n",
    "df_to_test['predict'] = predict(df_to_test, model_cnn)\n",
    "df_to_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность на случайной выборке составила: 89.0625%\n"
     ]
    }
   ],
   "source": [
    "acc = (df_to_test.target == df_to_test.predict).sum() / len(df_to_test) * 100\n",
    "print(f'Точность на случайной выборке составила: {acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
